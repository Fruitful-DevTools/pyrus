{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyrus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRING MANAGER\n",
    "\n",
    "This module provides a series of operations that can be used to manage string data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### string_normaliser\n",
    "\n",
    "This function normalises string data by performing several operations:\n",
    "- Converting all characters to lowercase\n",
    "- Removing leading/trailing white space\n",
    "- Removing double spaces\n",
    "- Replacing accented and special characters with their ASCII equivalents\n",
    "\n",
    "Args:\n",
    "    string (str): The string to be normalised.\n",
    "\n",
    "Returns:\n",
    "    normalised_string (str): The normalized string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "whitespace = string.whitespace\n",
    "\n",
    "def string_normaliser(string, normalise_encoding=False):\n",
    "\n",
    "    \n",
    "    # Turn the entire string to lowercase\n",
    "    lowercase_string = string.lower()\n",
    "\n",
    "    if normalise_encoding:\n",
    "        # Transform string into canonical representation\n",
    "        unicode_normalised_string = unicodedata.normalize(\n",
    "            'NFKD', lowercase_string)\n",
    "\n",
    "        # Encode string into ASCII format,\n",
    "        # Ignore letters that can't be turned into ASCII\n",
    "        lowercase_string = unidecode(unicode_normalised_string)\n",
    "    \n",
    "    replaced_string = lowercase_string.replace('@', '').replace('#', '')\n",
    "    # Strip the string of all whitespace\n",
    "    stripped_string = replaced_string.strip()\n",
    "    stripped_string = stripped_string.replace(whitespace, '')\n",
    "    \n",
    "\n",
    "    # Remove special characters from the string\n",
    "    pattern = re.compile(r'[^\\p{L}\\s\\d@#]')\n",
    "    special_character_removed_string = pattern.sub('', stripped_string)\n",
    "\n",
    "    \n",
    "    # While the string contains double spaces\n",
    "    # This ensures triple and more spaces are replaced\n",
    "    while '  ' in special_character_removed_string:\n",
    "        # Turn double spaces into single spaces\n",
    "       special_character_removed_string = special_character_removed_string.replace('  ', ' ')\n",
    "\n",
    "    normalised_string = special_character_removed_string\n",
    "\n",
    "    return normalised_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### string_normaliser: tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1e8091f58d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class StringNormaliserTests(unittest.TestCase):\n",
    "\n",
    "    def subtester(self, test_values):\n",
    "        \n",
    "        for value, expected_result in test_values:\n",
    "            with self.subTest(value=value):\n",
    "                result = string_normaliser(value)\n",
    "                self.assertEqual(result, expected_result)\n",
    "    \n",
    "    def test_case_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Hello World!\", \"hello world\"),\n",
    "            (\"ThIs Is A MiXeD CaSe StRiNg\", \"this is a mixed case string\"),\n",
    "            (\"Áccéntéd Cháráctérs\", \"áccéntéd cháráctérs\"),\n",
    "            (\"ALL UPPERCASE\", \"all uppercase\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "    \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_whitespace_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"   Remove  extra  spaces   \", \"remove extra spaces\"),\n",
    "            (\"  Leading and trailing spaces  \", \"leading and trailing spaces\"),\n",
    "            (\"    \", \"\"),  # All whitespace, expect empty string\n",
    "            (\"\", \"\")\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_double_space_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"This  has  double  spaces\", \"this has double spaces\"),\n",
    "            (\"No  double  spaces\", \"no double spaces\"),\n",
    "            (\"Single spaces\", \"single spaces\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_encoding_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Thís Štríng Hás Áccénted Characters\", \"thís štríng hás áccénted characters\"),\n",
    "            (\"Ünicöde Äscii Êncoding\", \"ünicöde äscii êncoding\"),\n",
    "            (\"Keep 1234567890 digits\", \"keep 1234567890 digits\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_special_char_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Hello@# World!\", \"hello world\"),\n",
    "            (\"Remove !@#$ special %^&* characters\", \"remove special characters\"),\n",
    "            (\"Keep digits 1234567890\", \"keep digits 1234567890\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_combined_normaisation(self):\n",
    "        test_values = [\n",
    "            # Normal test values\n",
    "            (\"Hello World!\", \"hello world\"),\n",
    "            (\"   Remove  extra  spaces   \", \"remove extra spaces\"),\n",
    "            (\"Thís Štríng Hás Áccénted Characters\", \"thís štríng hás áccénted characters\"),\n",
    "            (\"Ünicöde Äscii Êncoding\", \"ünicöde äscii êncoding\"),\n",
    "            (\"Hello@# World!\", \"hello world\"),\n",
    "            (\"Remove !@#$ special %^&* characters\", \"remove special characters\"),\n",
    "            (\"Keep digits 1234567890\", \"keep digits 1234567890\"),\n",
    "\n",
    "            # Extreme test values\n",
    "            (\"    \", \"\"),  # All whitespace, expect empty string\n",
    "            (\"!@#$%^&*()_+\", \"\"),  # All special characters, expect empty string\n",
    "            (\"ÁČÇÈÑTÉÐ ßÞÉÇÏÀL ÇHÁRÁÇTÉRS\", \"áčçèñtéð ßþéçïàl çháráçtérs\"),\n",
    "            (\"ÛÑÎÇØÐÊ ÄŠÇÏÏ ÊÑÇØÐÏÑG\", \"ûñîçøðê äšçïï êñçøðïñg\"),\n",
    "            (\"     ÛÑÎÇØÐÊ   \", \"ûñîçøðê\"),  # Leading/trailing whitespace with accented characters\n",
    "            (\"\\n    ÛÑÎÇØÐÊ     ÄŠÇÏÏ     ÊÑÇØÐÏÑG    \", \"ûñîçøðê äšçïï êñçøðïñg\"),  # Multiple operations with accented characters and whitespace\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presneter - Articles, Conjunctions and Prepositions\n",
    "\n",
    "Conventions on Articles, Conjunctions, Prepositions:\n",
    "\n",
    "Group 1: Articles, conjunctions, and prepositions are usually not capitalized in titles, unless they are the first word or part of a proper noun.\n",
    "\n",
    "French\n",
    "\n",
    "Group 2: Articles, conjunctions, and prepositions are typically not capitalized unless they are the first or last word, or if they have four or more letters.\n",
    "\n",
    "German\n",
    "Dutch\n",
    "\n",
    "Group 3: Articles, conjunctions, and prepositions are generally not capitalized in titles, unless they are the first or last word, or if they are stressed as part of the title's style or emphasis.\n",
    "\n",
    "Spanish\n",
    "\n",
    "Group 4: Articles, conjunctions, and prepositions are typically not capitalized in titles, except when they are the first or last word, or if they have special emphasis or are part of proper nouns.\n",
    "\n",
    "Italian\n",
    "Portuguese\n",
    "\n",
    "Group 5: Articles, conjunctions, and prepositions are generally not capitalized in titles unless they are the first or last word, or if they have special emphasis or are part of proper nouns.\n",
    "\n",
    "Norwegian\n",
    "Swedish\n",
    "Danish\n",
    "Finnish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['da', 'de', 'en', 'es', 'fi', 'fr', 'it', 'nl', 'no', 'pt', 'sv']\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from collections import ChainMap\n",
    "import csv\n",
    "\n",
    "\n",
    "class Librarian:\n",
    "    LIBRARY = 'library/'\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @contextmanager\n",
    "    def get_collection(self, collection):\n",
    "\n",
    "        file = None\n",
    "\n",
    "        try:\n",
    "            file = open(collection, 'r', newline='', encoding='utf-8-sig')\n",
    "            yield file\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        finally:\n",
    "            if file:\n",
    "                file.close()\n",
    "\n",
    "    def lookup(self, collection: str, *keys: str):\n",
    "\n",
    "        collection_filepath = f'{self.LIBRARY}{collection}.csv'\n",
    "\n",
    "        with self.get_collection(collection_filepath) as file:\n",
    "            open_file = csv.reader(file)\n",
    "            data_dict = self.create_dict(open_file)\n",
    "            result_map = ChainMap(data_dict)\n",
    "\n",
    "            for key in keys:\n",
    "                if key in result_map:\n",
    "                    result_map = result_map[key]\n",
    "\n",
    "            return result_map\n",
    "    \n",
    "    def create_dict(self, open_file):\n",
    "        \n",
    "        data_dict = {}\n",
    "        \n",
    "        headers = next(open_file)\n",
    "        for header in headers:\n",
    "            data_dict[header] = []  # Initialize empty lists for each header\n",
    "\n",
    "        for row in open_file:\n",
    "            for header, value in zip(headers, row):\n",
    "                data_dict[header].append(value)\n",
    "\n",
    "        return data_dict\n",
    "    \n",
    "    def create_datamap(self, open_file):\n",
    "        \n",
    "    def extract_data_from_csv(reader):\n",
    "        rows = list(reader)\n",
    "        parent_keys = rows[0]\n",
    "        child_dict = {}\n",
    "\n",
    "        for row in rows[1:]:\n",
    "            child_key = row[0]\n",
    "            child_value = row[1]\n",
    "            child_dict[child_key] = child_value\n",
    "\n",
    "        parent_dict = dict(zip(parent_keys, [child_dict] * len(parent_keys)))\n",
    "        data_map = ChainMap(parent_dict, child_dict)\n",
    "        return data_map\n",
    "\n",
    "librarian = Librarian()\n",
    "\n",
    "print(librarian.lookup('articles_conjunctions_prepositions', 'language'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
