{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyrus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRING MANAGER\n",
    "\n",
    "This module provides a series of operations that can be used to manage string data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### string_normaliser\n",
    "\n",
    "This function normalises string data by performing several operations:\n",
    "- Converting all characters to lowercase\n",
    "- Removing leading/trailing white space\n",
    "- Removing double spaces\n",
    "- Replacing accented and special characters with their ASCII equivalents\n",
    "\n",
    "Args:\n",
    "    string (str): The string to be normalised.\n",
    "\n",
    "Returns:\n",
    "    normalised_string (str): The normalized string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "whitespace = string.whitespace\n",
    "\n",
    "def string_normaliser(string, normalise_encoding=False):\n",
    "\n",
    "    \n",
    "    # Turn the entire string to lowercase\n",
    "    lowercase_string = string.lower()\n",
    "\n",
    "    if normalise_encoding:\n",
    "        # Transform string into canonical representation\n",
    "        unicode_normalised_string = unicodedata.normalize(\n",
    "            'NFKD', lowercase_string)\n",
    "\n",
    "        # Encode string into ASCII format,\n",
    "        # Ignore letters that can't be turned into ASCII\n",
    "        lowercase_string = unidecode(unicode_normalised_string)\n",
    "    \n",
    "    replaced_string = lowercase_string.replace('@', '').replace('#', '')\n",
    "    # Strip the string of all whitespace\n",
    "    stripped_string = replaced_string.strip()\n",
    "    stripped_string = stripped_string.replace(whitespace, '')\n",
    "    \n",
    "\n",
    "    # Remove special characters from the string\n",
    "    pattern = re.compile(r'[^\\p{L}\\s\\d@#]')\n",
    "    special_character_removed_string = pattern.sub('', stripped_string)\n",
    "\n",
    "    \n",
    "    # While the string contains double spaces\n",
    "    # This ensures triple and more spaces are replaced\n",
    "    while '  ' in special_character_removed_string:\n",
    "        # Turn double spaces into single spaces\n",
    "       special_character_removed_string = special_character_removed_string.replace('  ', ' ')\n",
    "\n",
    "    normalised_string = special_character_removed_string\n",
    "\n",
    "    return normalised_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### string_normaliser: tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x105cc5cf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class StringNormaliserTests(unittest.TestCase):\n",
    "\n",
    "    def subtester(self, test_values):\n",
    "        \n",
    "        for value, expected_result in test_values:\n",
    "            with self.subTest(value=value):\n",
    "                result = string_normaliser(value)\n",
    "                self.assertEqual(result, expected_result)\n",
    "    \n",
    "    def test_case_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Hello World!\", \"hello world\"),\n",
    "            (\"ThIs Is A MiXeD CaSe StRiNg\", \"this is a mixed case string\"),\n",
    "            (\"Áccéntéd Cháráctérs\", \"áccéntéd cháráctérs\"),\n",
    "            (\"ALL UPPERCASE\", \"all uppercase\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "    \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_whitespace_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"   Remove  extra  spaces   \", \"remove extra spaces\"),\n",
    "            (\"  Leading and trailing spaces  \", \"leading and trailing spaces\"),\n",
    "            (\"    \", \"\"),  # All whitespace, expect empty string\n",
    "            (\"\", \"\")\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_double_space_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"This  has  double  spaces\", \"this has double spaces\"),\n",
    "            (\"No  double  spaces\", \"no double spaces\"),\n",
    "            (\"Single spaces\", \"single spaces\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_encoding_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Thís Štríng Hás Áccénted Characters\", \"thís štríng hás áccénted characters\"),\n",
    "            (\"Ünicöde Äscii Êncoding\", \"ünicöde äscii êncoding\"),\n",
    "            (\"Keep 1234567890 digits\", \"keep 1234567890 digits\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_special_char_normalisation(self):\n",
    "        test_values = [\n",
    "            (\"Hello@# World!\", \"hello world\"),\n",
    "            (\"Remove !@#$ special %^&* characters\", \"remove special characters\"),\n",
    "            (\"Keep digits 1234567890\", \"keep digits 1234567890\"),\n",
    "            (\"\", \"\"),  # Empty string should remain the same\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "    def test_combined_normaisation(self):\n",
    "        test_values = [\n",
    "            # Normal test values\n",
    "            (\"Hello World!\", \"hello world\"),\n",
    "            (\"   Remove  extra  spaces   \", \"remove extra spaces\"),\n",
    "            (\"Thís Štríng Hás Áccénted Characters\", \"thís štríng hás áccénted characters\"),\n",
    "            (\"Ünicöde Äscii Êncoding\", \"ünicöde äscii êncoding\"),\n",
    "            (\"Hello@# World!\", \"hello world\"),\n",
    "            (\"Remove !@#$ special %^&* characters\", \"remove special characters\"),\n",
    "            (\"Keep digits 1234567890\", \"keep digits 1234567890\"),\n",
    "\n",
    "            # Extreme test values\n",
    "            (\"    \", \"\"),  # All whitespace, expect empty string\n",
    "            (\"!@#$%^&*()_+\", \"\"),  # All special characters, expect empty string\n",
    "            (\"ÁČÇÈÑTÉÐ ßÞÉÇÏÀL ÇHÁRÁÇTÉRS\", \"áčçèñtéð ßþéçïàl çháráçtérs\"),\n",
    "            (\"ÛÑÎÇØÐÊ ÄŠÇÏÏ ÊÑÇØÐÏÑG\", \"ûñîçøðê äšçïï êñçøðïñg\"),\n",
    "            (\"     ÛÑÎÇØÐÊ   \", \"ûñîçøðê\"),  # Leading/trailing whitespace with accented characters\n",
    "            (\"\\n    ÛÑÎÇØÐÊ     ÄŠÇÏÏ     ÊÑÇØÐÏÑG    \", \"ûñîçøðê äšçïï êñçøðïñg\"),  # Multiple operations with accented characters and whitespace\n",
    "        ]\n",
    "        \n",
    "        self.subtester(test_values)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "librarian.py - A module for managing collections and performing lookups in a library.\n",
    "\n",
    "This module provides the `Librarian` class, which allows you to interact with collections stored as CSV files in a library.\n",
    "\n",
    "Classes:\n",
    "    Librarian: A class for managing collections and performing lookups in a library.\n",
    "\n",
    "Usage:\n",
    "    from librarian import Librarian\n",
    "\n",
    "    librarian = Librarian()\n",
    "    result = librarian.lookup(\"my_collection\", \"key1\", \"key2\")\n",
    "\n",
    "    # The `lookup` method performs a lookup operation on a collection, using one or more keys.\n",
    "\n",
    "\"\"\"\n",
    "import csv\n",
    "from collections import ChainMap\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, List, Tuple, Generator, Any\n",
    "\n",
    "\n",
    "class Librarian:\n",
    "    \"\"\"\n",
    "    A class for managing collections and performing lookups in a library.\n",
    "\n",
    "    Attributes:\n",
    "        LIBRARY (str): The path to the library directory.\n",
    "\n",
    "    Methods:\n",
    "        get_collection: Context manager for accessing a collection file.\n",
    "        lookup: Perform a lookup operation on a collection.\n",
    "        extrapolate_dicts: Extrapolate dictionaries from an open CSV file.\n",
    "        create_datamap: Create a ChainMap data structure from dictionaries.\n",
    "    \"\"\"\n",
    "    LIBRARY = 'library/'\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @contextmanager\n",
    "    def get_collection(self, collection: str) -> Generator:\n",
    "        \"\"\"\n",
    "        Context manager to open a collection file.\n",
    "\n",
    "        Args:\n",
    "            collection (str): The name of the collection.\n",
    "\n",
    "        Yields:\n",
    "            file: The opened file object.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs while opening the file.\n",
    "        \"\"\"\n",
    "        file = None\n",
    "\n",
    "        try:\n",
    "            file = open(collection, 'r', newline='', encoding='utf-8-sig')\n",
    "            yield file\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        finally:\n",
    "            if file:\n",
    "                file.close()\n",
    "\n",
    "    def lookup(self, collection: str, *keys: str):\n",
    "        \"\"\"\n",
    "        Perform a lookup operation on a collection.\n",
    "\n",
    "        Args:\n",
    "            collection (str): The name of the collection.\n",
    "            *keys (str): Variable number of keys for the lookup.\n",
    "\n",
    "        Returns:\n",
    "            The result of the lookup operation.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs while performing the lookup.\n",
    "        \"\"\"\n",
    "        collection_filepath = f'{self.LIBRARY}{collection}.csv'\n",
    "\n",
    "        with self.get_collection(collection_filepath) as file:\n",
    "            rows: List[List[str]] = list(csv.reader(file))\n",
    "            parent_dict, child_dict, rows, parent_keys = self.extrapolate_dicts(rows)\n",
    "            result_map = self.create_datamap(parent_dict, child_dict, rows, parent_keys)\n",
    "            \n",
    "            for key in keys:\n",
    "                if key in result_map:\n",
    "                    result_map = result_map[key]\n",
    "\n",
    "            return result_map\n",
    "\n",
    "    def extrapolate_dicts(self, rows) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], List[List[str]], List[str]]:\n",
    "        \"\"\"\n",
    "        Extrapolate dictionaries from an open CSV file.\n",
    "\n",
    "        Args:\n",
    "            open_file (TextIO): The open text file object.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - parent_dict (Dict[str, Dict[str, str]]): The parent dictionary.\n",
    "            - child_dict (Dict[str, str]): The child dictionary.\n",
    "            - rows (List[List[str]]): The rows of the CSV file.\n",
    "            - parent_keys (List[str]): The keys of the parent dictionary.\n",
    "        \"\"\"\n",
    "        parent_keys: List[str] = rows[0]\n",
    "        child_dict: Dict[str, str] = {}\n",
    "\n",
    "        for row in rows[1:]:\n",
    "            child_key = row[0]\n",
    "            child_value = row[1]\n",
    "            child_dict[child_key] = child_value\n",
    "\n",
    "        parent_dict: Dict[str, Dict[str, str]] = dict(zip(parent_keys, [child_dict] * len(parent_keys)))\n",
    "        return parent_dict, child_dict, rows, parent_keys\n",
    "\n",
    "    def create_datamap(self, parent_dict: Dict[str, Dict[str, Any]], child_dict: Dict[str, Any], rows: List[List[str]], parent_keys: List[str]) -> ChainMap:\n",
    "        \"\"\"\n",
    "        Create a ChainMap data structure from dictionaries.\n",
    "\n",
    "        Args:\n",
    "            parent_dict (Dict[str, Dict[str, Any]]): The parent dictionary.\n",
    "            child_dict (Dict[str, Any]): The child dictionary.\n",
    "            rows (List[List[str]]): The rows of the CSV file.\n",
    "            parent_keys (List[str]): The keys of the parent dictionary.\n",
    "\n",
    "        Returns:\n",
    "            ChainMap: The created ChainMap data structure.\n",
    "        \"\"\"\n",
    "        for row in rows[1:]:\n",
    "            child_key = row[0]\n",
    "            child_value = row[1]\n",
    "            child_dict[child_key] = child_value\n",
    "\n",
    "        parent_dict = dict(zip(parent_keys, [child_dict] * len(parent_keys)))\n",
    "        data_map: ChainMap = ChainMap(parent_dict, child_dict)\n",
    "        return data_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librarian = Librarian()\n",
    "print(librarian.lookup('articles_conjunctions_prepositions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......FF.FEE\n",
      "======================================================================\n",
      "ERROR: test_lookup_key_not_found (__main__.TestLibrarian)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/90/7k5gmw150tnbl3nn8np374kw0000gn/T/ipykernel_11035/1685998256.py\", line 46, in test_lookup_key_not_found\n",
      "    result = self.librarian.lookup(collection, *keys)\n",
      "  File \"/Users/charliemarshall/Documents/GitHub/iammokzeeee personal tools/filemanagement/pyrus/src/pyrus/librarian.py\", line 36, in lookup\n",
      "    unmapped_dicts = self.extrapolate_dicts(open_file)\n",
      "  File \"/Users/charliemarshall/Documents/GitHub/iammokzeeee personal tools/filemanagement/pyrus/src/pyrus/librarian.py\", line 48, in extrapolate_dicts\n",
      "    parent_keys = rows[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_lookup_success (__main__.TestLibrarian)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/90/7k5gmw150tnbl3nn8np374kw0000gn/T/ipykernel_11035/1685998256.py\", line 34, in test_lookup_success\n",
      "    result = self.librarian.lookup(collection, *keys)\n",
      "  File \"/Users/charliemarshall/Documents/GitHub/iammokzeeee personal tools/filemanagement/pyrus/src/pyrus/librarian.py\", line 36, in lookup\n",
      "    unmapped_dicts = self.extrapolate_dicts(open_file)\n",
      "  File \"/Users/charliemarshall/Documents/GitHub/iammokzeeee personal tools/filemanagement/pyrus/src/pyrus/librarian.py\", line 48, in extrapolate_dicts\n",
      "    parent_keys = rows[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_create_datamap (__main__.TestLibrarian)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/90/7k5gmw150tnbl3nn8np374kw0000gn/T/ipykernel_11035/1685998256.py\", line 100, in test_create_datamap\n",
      "    self.assertEqual(datamap, expected_datamap)\n",
      "AssertionError: ChainMap({'parent_key': {'child1': 'value[146 chars]d2'}) != {'parent_key': {'child_key': 'child_value[38 chars]ue2'}\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_extrapolate_dicts (__main__.TestLibrarian)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/90/7k5gmw150tnbl3nn8np374kw0000gn/T/ipykernel_11035/1685998256.py\", line 69, in test_extrapolate_dicts\n",
      "    self.assertEqual(parent_dict, expected_parent_dict)\n",
      "AssertionError: {'parent_key': {'parent1': 'child1', 'parent2': 'child2'},[111 chars]d2'}} != {'parent_key': {'child_key': 'child_value'}}\n",
      "+ {'parent_key': {'child_key': 'child_value'}}\n",
      "- {'child_key': {'parent1': 'child1', 'parent2': 'child2'},\n",
      "-  'child_value': {'parent1': 'child1', 'parent2': 'child2'},\n",
      "-  'parent_key': {'parent1': 'child1', 'parent2': 'child2'}}\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_get_collection_success (__main__.TestLibrarian)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/90/7k5gmw150tnbl3nn8np374kw0000gn/T/ipykernel_11035/1685998256.py\", line 14, in test_get_collection_success\n",
      "    mock_file.assert_called_once_with(expected_file_path, 'r', newline='', encoding='utf-8-sig')\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/unittest/mock.py\", line 931, in assert_called_once_with\n",
      "    return self.assert_called_with(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/unittest/mock.py\", line 919, in assert_called_with\n",
      "    raise AssertionError(_error_message()) from cause\n",
      "AssertionError: expected call not found.\n",
      "Expected: open('library/my_collection.csv', 'r', newline='', encoding='utf-8-sig')\n",
      "Actual: open('my_collection', 'r', newline='', encoding='utf-8-sig')\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.018s\n",
      "\n",
      "FAILED (failures=3, errors=2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x10a00ffd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import mock_open, patch\n",
    "\n",
    "class TestLibrarian(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.librarian = Librarian()\n",
    "\n",
    "    def test_get_collection_success(self):\n",
    "        collection = 'my_collection'\n",
    "        expected_file_path = 'library/my_collection.csv'\n",
    "\n",
    "        with patch('builtins.open', mock_open()) as mock_file:\n",
    "            with self.librarian.get_collection(collection) as file:\n",
    "                mock_file.assert_called_once_with(expected_file_path, 'r', newline='', encoding='utf-8-sig')\n",
    "\n",
    "    def test_get_collection_exception(self):\n",
    "        collection = 'non_existent_collection'\n",
    "        expected_file_path = 'library/non_existent_collection.csv'\n",
    "\n",
    "        with patch('builtins.open', side_effect=FileNotFoundError):\n",
    "            with self.assertRaises(Exception):\n",
    "                with self.librarian.get_collection(collection):\n",
    "                    pass\n",
    "\n",
    "    def test_lookup_success(self):\n",
    "        collection = 'my_collection'\n",
    "        keys = ('key1', 'key2')\n",
    "        expected_result = 'lookup_result'\n",
    "\n",
    "        with patch('builtins.open', mock_open()) as mock_file:\n",
    "            mock_reader = mock_file.return_value.__enter__.return_value\n",
    "            mock_reader.__iter__.return_value = [['parent_key', 'child_key', expected_result]]\n",
    "            \n",
    "            result = self.librarian.lookup(collection, *keys)\n",
    "            \n",
    "            self.assertEqual(result, expected_result)\n",
    "\n",
    "    def test_lookup_key_not_found(self):\n",
    "        collection = 'my_collection'\n",
    "        keys = ('key1', 'key2')\n",
    "        \n",
    "        with patch('builtins.open', mock_open()) as mock_file:\n",
    "            mock_reader = mock_file.return_value.__enter__.return_value\n",
    "            mock_reader.__iter__.return_value = [['parent_key', 'child_key', 'lookup_result']]\n",
    "            \n",
    "            result = self.librarian.lookup(collection, *keys)\n",
    "            \n",
    "            self.assertIsNone(result)\n",
    "\n",
    "    def test_extrapolate_dicts(self):\n",
    "        rows = [\n",
    "            ['parent_key', 'child_key', 'child_value'],\n",
    "            ['parent1', 'child1', 'value1'],\n",
    "            ['parent2', 'child2', 'value2']\n",
    "        ]\n",
    "        expected_parent_dict = {\n",
    "            'parent_key': {\n",
    "                'child_key': 'child_value'\n",
    "            }\n",
    "        }\n",
    "        expected_child_dict = {\n",
    "            'child1': 'value1',\n",
    "            'child2': 'value2'\n",
    "        }\n",
    "        expected_parent_keys = ['parent_key']\n",
    "\n",
    "        parent_dict, child_dict, _, parent_keys = self.librarian.extrapolate_dicts(rows)\n",
    "\n",
    "        self.assertEqual(parent_dict, expected_parent_dict)\n",
    "        self.assertEqual(child_dict, expected_child_dict)\n",
    "        self.assertEqual(parent_keys, expected_parent_keys)\n",
    "\n",
    "    def test_create_datamap(self):\n",
    "        parent_dict = {\n",
    "            'parent_key': {\n",
    "                'child_key': 'child_value'\n",
    "            }\n",
    "        }\n",
    "        child_dict = {\n",
    "            'child1': 'value1',\n",
    "            'child2': 'value2'\n",
    "        }\n",
    "        rows = [\n",
    "            ['parent_key', 'child_key', 'child_value'],\n",
    "            ['parent1', 'child1', 'value1'],\n",
    "            ['parent2', 'child2', 'value2']\n",
    "        ]\n",
    "        parent_keys = ['parent_key']\n",
    "\n",
    "        expected_datamap = {\n",
    "            'parent_key': {\n",
    "                'child_key': 'child_value'\n",
    "            },\n",
    "            'child1': 'value1',\n",
    "            'child2': 'value2'\n",
    "        }\n",
    "\n",
    "        datamap = self.librarian.create_datamap(parent_dict, child_dict, rows, parent_keys)\n",
    "\n",
    "        self.assertEqual(datamap, expected_datamap)\n",
    "\n",
    "unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presneter - Articles, Conjunctions and Prepositions\n",
    "\n",
    "Conventions on Articles, Conjunctions, Prepositions:\n",
    "\n",
    "Group 1: Articles, conjunctions, and prepositions are usually not capitalized in titles, unless they are the first word or part of a proper noun.\n",
    "\n",
    "French\n",
    "\n",
    "Group 2: Articles, conjunctions, and prepositions are typically not capitalized unless they are the first or last word, or if they have four or more letters.\n",
    "\n",
    "German\n",
    "Dutch\n",
    "\n",
    "Group 3: Articles, conjunctions, and prepositions are generally not capitalized in titles, unless they are the first or last word, or if they are stressed as part of the title's style or emphasis.\n",
    "\n",
    "Spanish\n",
    "\n",
    "Group 4: Articles, conjunctions, and prepositions are typically not capitalized in titles, except when they are the first or last word, or if they have special emphasis or are part of proper nouns.\n",
    "\n",
    "Italian\n",
    "Portuguese\n",
    "\n",
    "Group 5: Articles, conjunctions, and prepositions are generally not capitalized in titles unless they are the first or last word, or if they have special emphasis or are part of proper nouns.\n",
    "\n",
    "Norwegian\n",
    "Swedish\n",
    "Danish\n",
    "Finnish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
